{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj8Li3GGFWsH4OZhL6HGSe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrishaAggarwal/Machine_Learning_Assignments/blob/main/ML_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGQNFHfigRSx",
        "outputId": "035cf83d-b666-4ca2-f835-c497a83f4f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== All Results ===\n",
            "\n",
            "    Learning Rate        Lambda          Cost      R2 Score\n",
            "0          0.0001  1.000000e-15  1.890233e+01  7.415456e-01\n",
            "1          0.0001  1.000000e-10  1.890233e+01  7.415456e-01\n",
            "2          0.0001  1.000000e-05  1.890233e+01  7.415456e-01\n",
            "3          0.0001  1.000000e-03  1.890234e+01  7.415456e-01\n",
            "4          0.0001  0.000000e+00  1.890233e+01  7.415456e-01\n",
            "5          0.0001  1.000000e+00  1.891078e+01  7.415015e-01\n",
            "6          0.0001  1.000000e+01  1.898674e+01  7.411045e-01\n",
            "7          0.0001  2.000000e+01  1.907096e+01  7.406635e-01\n",
            "8          0.0010  1.000000e-15  3.377093e-01  9.953825e-01\n",
            "9          0.0010  1.000000e-10  3.377093e-01  9.953825e-01\n",
            "10         0.0010  1.000000e-05  3.377095e-01  9.953825e-01\n",
            "11         0.0010  1.000000e-03  3.377306e-01  9.953825e-01\n",
            "12         0.0010  0.000000e+00  3.377093e-01  9.953825e-01\n",
            "13         0.0010  1.000000e+00  3.590258e-01  9.953817e-01\n",
            "14         0.0010  1.000000e+01  5.503142e-01  9.953675e-01\n",
            "15         0.0010  2.000000e+01  7.616798e-01  9.953360e-01\n",
            "16         0.0100  1.000000e-15  3.040754e-01  9.958423e-01\n",
            "17         0.0100  1.000000e-10  3.040754e-01  9.958423e-01\n",
            "18         0.0100  1.000000e-05  3.040756e-01  9.958423e-01\n",
            "19         0.0100  1.000000e-03  3.040974e-01  9.958423e-01\n",
            "20         0.0100  0.000000e+00  3.040754e-01  9.958423e-01\n",
            "21         0.0100  1.000000e+00  3.261023e-01  9.958375e-01\n",
            "22         0.0100  1.000000e+01  5.230138e-01  9.957886e-01\n",
            "23         0.0100  2.000000e+01  7.392829e-01  9.957232e-01\n",
            "24         0.1000  1.000000e-15  1.608501e-01  9.978007e-01\n",
            "25         0.1000  1.000000e-10  1.608501e-01  9.978007e-01\n",
            "26         0.1000  1.000000e-05  1.608506e-01  9.978007e-01\n",
            "27         0.1000  1.000000e-03  1.608964e-01  9.978006e-01\n",
            "28         0.1000  0.000000e+00  1.608501e-01  9.978007e-01\n",
            "29         0.1000  1.000000e+00  2.046574e-01  9.976872e-01\n",
            "30         0.1000  1.000000e+01  4.850489e-01  9.968378e-01\n",
            "31         0.1000  2.000000e+01  7.239741e-01  9.963115e-01\n",
            "32         1.0000  1.000000e-15  1.124809e+11 -1.537968e+09\n",
            "33         1.0000  1.000000e-10  1.124809e+11 -1.537968e+09\n",
            "34         1.0000  1.000000e-05  1.124809e+11 -1.537968e+09\n",
            "35         1.0000  1.000000e-03  1.124813e+11 -1.537973e+09\n",
            "36         1.0000  0.000000e+00  1.124809e+11 -1.537968e+09\n",
            "37         1.0000  1.000000e+00  1.129116e+11 -1.543405e+09\n",
            "38         1.0000  1.000000e+01  1.168559e+11 -1.593124e+09\n",
            "39         1.0000  2.000000e+01  1.213851e+11 -1.650056e+09\n",
            "40        10.0000  1.000000e-15  5.161200e+12 -7.056985e+10\n",
            "41        10.0000  1.000000e-10  5.161200e+12 -7.056985e+10\n",
            "42        10.0000  1.000000e-05  5.161200e+12 -7.056985e+10\n",
            "43        10.0000  1.000000e-03  5.161195e+12 -7.056976e+10\n",
            "44        10.0000  0.000000e+00  5.161200e+12 -7.056985e+10\n",
            "45        10.0000  1.000000e+00  5.155856e+12 -7.047512e+10\n",
            "46        10.0000  1.000000e+01  5.107705e+12 -6.962425e+10\n",
            "47        10.0000  2.000000e+01  5.054087e+12 -6.868255e+10\n",
            "\n",
            "=== Best Hyperparameters ===\n",
            "Learning Rate    1.000000e-01\n",
            "Lambda           1.000000e-15\n",
            "Cost             1.608501e-01\n",
            "R2 Score         9.978007e-01\n",
            "Name: 24, dtype: float64\n",
            "\n",
            "=== Final Model Performance ===\n",
            "Best Learning Rate   : 0.1\n",
            "Best Lambda          : 1e-15\n",
            "Final Cost Function  : 0.16085009725929875\n",
            "Final R2 Score       : 0.9978006728091807\n",
            "Final Weights        : [-0.00596835  2.21195523 -1.14593948  2.18669674  3.86306992  0.46484156\n",
            "  2.77591507  2.0556793 ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Generate Highly Correlated Dataset\n",
        "\n",
        "np.random.seed(42)\n",
        "n_samples = 500\n",
        "\n",
        "x_base = np.random.randn(n_samples)\n",
        "\n",
        "X = np.column_stack([\n",
        "    x_base,\n",
        "    x_base * 0.9 + np.random.randn(n_samples) * 0.1,\n",
        "    x_base * 1.1 + np.random.randn(n_samples) * 0.1,\n",
        "    x_base * 0.8 + np.random.randn(n_samples) * 0.1,\n",
        "    x_base * 1.05 + np.random.randn(n_samples) * 0.1,\n",
        "    x_base * 0.95 + np.random.randn(n_samples) * 0.1,\n",
        "    x_base * 1.2 + np.random.randn(n_samples) * 0.1\n",
        "])\n",
        "\n",
        "true_w = np.array([4, -3, 2.5, 5, -1, 3, 2])\n",
        "y = X.dot(true_w) + np.random.randn(n_samples) * 0.5\n",
        "\n",
        "# Add bias term\n",
        "X = np.c_[np.ones((n_samples, 1)), X]\n",
        "\n",
        "# 2. Ridge Regression using Gradient Descent (Stable Version)\n",
        "def ridge_gradient_descent(X, y, lr, lam, iterations=1000):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        y_pred = X.dot(w)\n",
        "\n",
        "        # Gradient\n",
        "        grad = (1/m) * X.T.dot(y_pred - y)\n",
        "        grad[1:] += (lam/m) * w[1:]\n",
        "\n",
        "        # Gradient clipping to prevent explosion\n",
        "        grad = np.clip(grad, -1e5, 1e5)\n",
        "\n",
        "        # Update rule\n",
        "        w -= lr * grad\n",
        "\n",
        "        # Check for instability\n",
        "        if np.any(np.isnan(w)) or np.any(np.isinf(w)):\n",
        "            return w, np.nan\n",
        "\n",
        "    # Final cost function\n",
        "    cost = (1/(2*m)) * np.sum((X.dot(w) - y)**2) + (lam/(2*m)) * np.sum(w[1:]**2)\n",
        "    return w, cost\n",
        "\n",
        "# 3. Hyperparameter Search\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "lambdas = [10**-15, 10**-10, 10**-5, 10**-3, 0, 1, 10, 20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        w, final_cost = ridge_gradient_descent(X, y, lr, lam)\n",
        "\n",
        "        if np.isnan(final_cost):\n",
        "            continue\n",
        "\n",
        "        y_pred = X.dot(w)\n",
        "\n",
        "        if np.any(np.isnan(y_pred)):\n",
        "            continue\n",
        "\n",
        "        r2 = r2_score(y, y_pred)\n",
        "\n",
        "        results.append([lr, lam, final_cost, r2])\n",
        "\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(results, columns=[\"Learning Rate\", \"Lambda\", \"Cost\", \"R2 Score\"])\n",
        "print(\"\\n=== All Results ===\\n\")\n",
        "print(df)\n",
        "\n",
        "# 4. Best Parameters\n",
        "\n",
        "best = df.loc[df[\"R2 Score\"].idxmax()]\n",
        "best_lr = best[\"Learning Rate\"]\n",
        "best_lam = best[\"Lambda\"]\n",
        "\n",
        "print(\"\\n=== Best Hyperparameters ===\")\n",
        "print(best)\n",
        "\n",
        "# Train final model\n",
        "final_w, final_cost = ridge_gradient_descent(X, y, best_lr, best_lam)\n",
        "final_r2 = r2_score(y, X.dot(final_w))\n",
        "\n",
        "print(\"\\n=== Final Model Performance ===\")\n",
        "print(\"Best Learning Rate   :\", best_lr)\n",
        "print(\"Best Lambda          :\", best_lam)\n",
        "print(\"Final Cost Function  :\", final_cost)\n",
        "print(\"Final R2 Score       :\", final_r2)\n",
        "print(\"Final Weights        :\", final_w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "#Using Another dataset instead of hitler since that file isnt opening\n",
        "# (a) Load & Preprocess Dataset\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load diabetes dataset\n",
        "data = load_diabetes()\n",
        "\n",
        "# Convert to DataFrame\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Missing values:\\n\", X.isna().sum())\n",
        "\n",
        "# (b) Fit Linear, Ridge, LASSO Regression\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# (c) Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Regularization parameters\n",
        "alpha_val = 0.5748\n",
        "\n",
        "# Initialize models\n",
        "lin_model = LinearRegression()\n",
        "ridge_model = Ridge(alpha=alpha_val)\n",
        "lasso_model = Lasso(alpha=alpha_val, max_iter=10000)\n",
        "\n",
        "# Fit models\n",
        "lin_model.fit(X_train_scaled, y_train)\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# (d) Model Evaluation\n",
        "def evaluate(model, X_test, y_test):\n",
        "    pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    return mse, r2\n",
        "\n",
        "lin_mse, lin_r2 = evaluate(lin_model, X_test_scaled, y_test)\n",
        "ridge_mse, ridge_r2 = evaluate(ridge_model, X_test_scaled, y_test)\n",
        "lasso_mse, lasso_r2 = evaluate(lasso_model, X_test_scaled, y_test)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n Model Performance\")\n",
        "print(f\"Linear Regression -> MSE: {lin_mse:.4f}, R2: {lin_r2:.4f}\")\n",
        "print(f\"Ridge Regression  -> MSE: {ridge_mse:.4f}, R2: {ridge_r2:.4f}\")\n",
        "print(f\"LASSO Regression  -> MSE: {lasso_mse:.4f}, R2: {lasso_r2:.4f}\")\n",
        "\n",
        "# Best model conclusion\n",
        "best = max([(\"Linear\", lin_r2), (\"Ridge\", ridge_r2), (\"LASSO\", lasso_r2)], key=lambda x: x[1])\n",
        "print(f\"\\nBest model: {best[0]} (highest R2 score)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLUuIqjqhL76",
        "outputId": "bf9f31ab-25ac-4d8b-e245-7e8cee2dd951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (442, 10)\n",
            "Missing values:\n",
            " age    0\n",
            "sex    0\n",
            "bmi    0\n",
            "bp     0\n",
            "s1     0\n",
            "s2     0\n",
            "s3     0\n",
            "s4     0\n",
            "s5     0\n",
            "s6     0\n",
            "dtype: int64\n",
            "\n",
            " Model Performance\n",
            "Linear Regression -> MSE: 2900.1936, R2: 0.4526\n",
            "Ridge Regression  -> MSE: 2894.7666, R2: 0.4536\n",
            "LASSO Regression  -> MSE: 2851.9489, R2: 0.4617\n",
            "\n",
            "Best model: LASSO (highest R2 score)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3\n",
        "# RidgeCV & LassoCV on California housing since boston dataset is removed from latest colab version\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load Boston Dataset\n",
        "data = fetch_california_housing()\n",
        "\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "print(\"Dataset Shape:\", X.shape)\n",
        "print(\"Missing Values:\\n\", X.isna().sum())\n",
        "\n",
        "# Train–Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Ridge Cross Validation\n",
        "alphas = np.logspace(-3, 3, 50)\n",
        "\n",
        "ridge_cv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error')\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "ridge_pred = ridge_cv.predict(X_test_scaled)\n",
        "\n",
        "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
        "ridge_r2 = r2_score(y_test, ridge_pred)\n",
        "\n",
        "print(\"\\n----- RidgeCV Results -----\")\n",
        "print(\"Best α:\", ridge_cv.alpha_)\n",
        "print(\"MSE:\", ridge_mse)\n",
        "print(\"R2 Score:\", ridge_r2)\n",
        "\n",
        "# Lasso Cross Validation\n",
        "lasso_cv = LassoCV(alphas=alphas, max_iter=5000, cv=5)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "lasso_pred = lasso_cv.predict(X_test_scaled)\n",
        "\n",
        "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
        "lasso_r2 = r2_score(y_test, lasso_pred)\n",
        "\n",
        "print(\"\\n----- LassoCV Results -----\")\n",
        "print(\"Best α:\", lasso_cv.alpha_)\n",
        "print(\"MSE:\", lasso_mse)\n",
        "print(\"R2 Score:\", lasso_r2)\n",
        "\n",
        "# Final Comparison\n",
        "print(\"\\n----- Final Comparison -----\")\n",
        "print(f\"RidgeCV -> MSE: {ridge_mse:.4f}, R2: {ridge_r2:.4f}\")\n",
        "print(f\"LassoCV -> MSE: {lasso_mse:.4f}, R2: {lasso_r2:.4f}\")\n",
        "\n",
        "if ridge_r2 > lasso_r2:\n",
        "    print(\"\\nBest Model: RidgeCV\")\n",
        "else:\n",
        "    print(\"\\nBest Model: LassoCV\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjNehV1LiBrB",
        "outputId": "7b4b0f08-52a0-461c-a115-5a5d73882174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (20640, 8)\n",
            "Missing Values:\n",
            " MedInc        0\n",
            "HouseAge      0\n",
            "AveRooms      0\n",
            "AveBedrms     0\n",
            "Population    0\n",
            "AveOccup      0\n",
            "Latitude      0\n",
            "Longitude     0\n",
            "dtype: int64\n",
            "\n",
            "----- RidgeCV Results -----\n",
            "Best α: 2.023589647725158\n",
            "MSE: 0.5558175016320879\n",
            "R2 Score: 0.5758442510228978\n",
            "\n",
            "----- LassoCV Results -----\n",
            "Best α: 0.001\n",
            "MSE: 0.5544913600832686\n",
            "R2 Score: 0.5768562568705682\n",
            "\n",
            "----- Final Comparison -----\n",
            "RidgeCV -> MSE: 0.5558, R2: 0.5758\n",
            "LassoCV -> MSE: 0.5545, R2: 0.5769\n",
            "\n",
            "Best Model: LassoCV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data            # 4 features\n",
        "y = iris.target          # 3 classes -> 0,1,2\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 2️. Sigmoid Function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# 3️. Train Logistic Regression for ONE class (binary)\n",
        "def train_binary_logistic(X, y, lr=0.1, epochs=2000):\n",
        "    m, n = X.shape\n",
        "    W = np.zeros(n)\n",
        "    b = 0\n",
        "\n",
        "    for i in range(epochs):\n",
        "        z = np.dot(X, W) + b\n",
        "        y_hat = sigmoid(z)\n",
        "\n",
        "        # gradients\n",
        "        dW = (1/m) * np.dot(X.T, (y_hat - y))\n",
        "        db = (1/m) * np.sum(y_hat - y)\n",
        "\n",
        "        # update\n",
        "        W -= lr * dW\n",
        "        b -= lr * db\n",
        "\n",
        "    return W, b\n",
        "\n",
        "# 4️. ONE VS REST TRAINING\n",
        "def train_ovr(X, y):\n",
        "    classes = np.unique(y)\n",
        "    weights = {}\n",
        "    biases = {}\n",
        "\n",
        "    for cls in classes:\n",
        "        print(f\"Training classifier for class {cls} vs rest...\")\n",
        "\n",
        "        # create binary labels: class = 1, rest = 0\n",
        "        y_binary = (y == cls).astype(int)\n",
        "\n",
        "        W, b = train_binary_logistic(X, y_binary)\n",
        "        weights[cls] = W\n",
        "        biases[cls] = b\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "# 5️. Making Predictions\n",
        "def predict_ovr(X, weights, biases):\n",
        "    class_scores = []\n",
        "\n",
        "    for cls in weights:\n",
        "        W = weights[cls]\n",
        "        b = biases[cls]\n",
        "\n",
        "        score = sigmoid(np.dot(X, W) + b)\n",
        "        class_scores.append(score)\n",
        "\n",
        "    # convert list to matrix → shape: (num_classes, num_samples)\n",
        "    class_scores = np.array(class_scores)\n",
        "\n",
        "    # choose class with highest probability\n",
        "    predictions = np.argmax(class_scores, axis=0)\n",
        "    return predictions\n",
        "\n",
        "# 6️. TRAIN OVR MODEL\n",
        "weights, biases = train_ovr(X_train, y_train)\n",
        "\n",
        "# 7️. EVALUATE MODEL\n",
        "y_pred = predict_ovr(X_test, weights, biases)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nPredictions:\", y_pred)\n",
        "print(\"True labels:\", y_test)\n",
        "print(\"\\nFINAL ACCURACY:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLCKJf7PkWEA",
        "outputId": "2c4aa3a1-c30b-430e-9f7a-a4e647836671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier for class 0 vs rest...\n",
            "Training classifier for class 1 vs rest...\n",
            "Training classifier for class 2 vs rest...\n",
            "\n",
            "Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "True labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "\n",
            "FINAL ACCURACY: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H4icO-xzlGb6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}